---
title: "Seattle Airbnb"
output: html_document
---

By Yiming Sun
Data source:https://www.kaggle.com/airbnb/seattle

The project hopes to use data to give Airbnb hosts some fact-supported instructions on how to become a super host in Seattle. The first part of the project is a logistic regression, and the second part is a word cloud visualization of hosts' introductions.
A report blog with insights could be found here: https://www.linkedin.com/pulse/how-become-airbnb-super-host-seattle-cynthia-yiming-sun/

#Data Preparation
This datasets have already been partially cleaned by excel( Removed columns and imputed missing value for neighbourhood column)
```{r}
library(readr)
listings <- read_csv('https://raw.githubusercontent.com/sunyiming2016/seattle_airbnb_superhost_classification/master/listings.csv',na='.')
```

check and visualize missing values
```{r}
library(Amelia)
missmap(listings[-1],col=c('grey','steelblue'),y.cex=0.5,x.cex=0.5)
```

Remove the rows with missing values
```{r}
listings=subset(listings, host_response_rate!="N/A")
listings=subset(listings, host_about!="")
listings=subset(listings, bathrooms!="")
listings=subset(listings, bedrooms!="")
listings=subset(listings, select=-c(review_scores_rating))
#it seems that there are lots of missing values for the 'review_scores_rating' variable, and we might run into endogeneity issues to put it into the superhost regression. So we remove this variable here.
```

Check variability
```{r}
library(tidyverse)
library(ggplot2)
listings %>% ggplot() + geom_bar(aes(host_response_time))
listings %>% ggplot() + geom_bar(aes(host_is_superhost))
listings %>% ggplot() + geom_bar(aes(neighbourhood_group_cleansed))
#all good here
```

##Let's use logistic regression to explore the key elements of becoming a super host in Seattle
```{r}
#generate a new column to show the time length of the host joined the airbnb
str(listings$host_since)
listings$host_since= as.Date(listings$host_since, format = "%m/%d/%Y")
length= difftime("0016-01-04",listings$host_since, unit = "days")
length<-round(length)
View(length)
listings<-cbind(listings,length)
```

transform the data type
```{r}
str(train.df)
listings$host_response_rate<-as.numeric(gsub("\\%", "",listings$host_response_rate))
listings$length<-as.numeric(listings$length)
listings$host_response_time<-as.factor(listings$host_response_time)
listings$neighbourhood_group_cleansed<-as.factor(listings$neighbourhood_group_cleansed)
listings$property_type<-as.factor(listings$property_type)
listings$room_type<-as.factor(listings$room_type)
listings$cancellation_policy<-as.factor(listings$cancellation_policy)
```

select predicting variables
```{r}
select.var <-c(8,9,10,11,15,16,17,18,19,22,23)
train.index <- sample(1:nrow(listings), 0.7*nrow(listings))
train.df <- listings[train.index, select.var]
```

test correlation between numeric variables
```{r}
test.var<-c(2,7,8,9,11)
testdata<-train.df[test.var]
cor(testdata)
#the bathrooms and bedrooms and beds those three factors are highly corelated, let's just keep one: bedrooms
#so we need to update the train.df and valid.df
select.var2 <-c(8,9,10,11,15,16,18,22,23)
train.index2 <- sample(1:nrow(listings), 0.7*nrow(listings))
train2.df <- listings[train.index, select.var2]
valid2.df <- listings[-train.index,select.var2]
str(train2.df)
```


set the base
```{r}
train2.df$host_response_time <- relevel(train2.df$host_response_time, ref = "within an hour")
train2.df$neighbourhood_group_cleansed<- relevel(train2.df$neighbourhood_group_cleansed, ref = "University District")
train2.df$property_type <- relevel(train2.df$property_type, ref = "Apartment")
train2.df$room_type<-relevel(train2.df$room_type,ref = "Entire home/apt")
train2.df$cancellation_policy<-relevel(train2.df$cancellation_policy,ref = "flexible")
str(train2.df)
```

transform prediction variable into 0/1
```{r}
train2.df$host_is_superhost <- as.factor (train2.df$host_is_superhost) 
levels(train2.df$host_is_superhost)<-c(0,1)
names(train2.df$host_is_superhost)[2] <- "is_superhost"

```

run the regression
```{r}
model <- glm(train2.df$host_is_superhost~.,family=binomial,data=train2.df)
summary(model)
#we could notice that a quicker response time and a stricter canllcelation policy would lead to a higher probablity of becoming a superhost. Host with properties in University District are the least possible to become a superhost. Property type cabin is significantly better than apartment,sharedroom type is significantly worse than entire home/apt. And the host's length of joining airbnb doesn't play an important role in effecting whether the host would become a superhost. And a surprising result is that more bedrooms tend to decrease the host's probability of becoming a superhost. I think it might be a lurking variable here. Perhaps more bedrooms tend to be more less affordable and that might be the reason to decrease the property's popularity. 
```

validation
```{r}
library(caret)
model.pred <- predict(model,valid2.df,type='response')
pred<-ifelse(model.pred>0.5,"t","f")
confusionMatrix(factor(pred),factor(valid2.df$host_is_superhost))
#our prediction power here is great!
```

#Let's create the word cloud now. 
install packages
```{r}
library(tm)
library(NLP)
library(wordcloud)
```

build a corpus for text mining
```{r}
as.character(listings$host_about)
library(stringr)
host=str_replace_all(listings$host_about,"[^[:alpha:]]", " ") 
corp <- Corpus(VectorSource(host))
inspect(corp)
```

clean and pre-process the text data
```{r}
corp <- tm_map(corp, removeNumbers)
corp<- tm_map(corp, removePunctuation)
corp <- tm_map(corp, stripWhitespace) 
corp<- tm_map(corp, tolower) #transform into low case
corp <- tm_map(corp, removeWords, stopwords("english")) 
corp<- tm_map(corp,removeWords, c("i","we","live","living","seattle","washington","airbnb"))#remove your own stop word
inspect(corp)
```

Generate the spreadsheet representation of the documents.
```{r}
tdm <- DocumentTermMatrix(corp)
inspect(tdm)
```

ready for the word cloud? I saw "love" on the first sight.
```{r}
m <- as.matrix(tdm)
# find out the importance of each term by summing up the tf-idf scores over the corpus
importance <- data.frame(sort(colSums(m),decreasing=TRUE))
set.seed(111)
wordcloud(rownames(importance), importance[,1], max.words=200, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
```

